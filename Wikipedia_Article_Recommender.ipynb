{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49c83f73",
   "metadata": {},
   "source": [
    "### Wikipedia Article Recommendation\n",
    "\n",
    "#### Task :-\n",
    "Suggest some random Wikipedia articles based on user preferences \n",
    "\n",
    "1. Take input from users their preferences, for example (movies, general knowledge, etc)\n",
    "2. Suggest an article if the user wants to read it show the article on the browser else repeat the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b5ada",
   "metadata": {},
   "source": [
    "#### Dataset Used :-\n",
    "\n",
    "For doing the task, we need a dataset that is having wikipedia's articles along with it's titles.Thus, we are going to download some random articles  from wikipedia using Python API.\n",
    "We have used only 10000 artices as of now as it's little bit time consuming but more articles we will use, better wil be our model.\n",
    "\n",
    "\n",
    "#### Metrics Used :-\n",
    "\n",
    "We are using **cosine similarity** metric for finding similarity between the features from wikipedia article.\n",
    "\n",
    "#### Why cosine similarity used?\n",
    "\n",
    "If two vectors are far apart they might be similar as their orientation is same .In cosine similarity, even if distances are large between two vectors but if their orientation is same, it will be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2575d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2d6f9",
   "metadata": {},
   "source": [
    "#### Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70523b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of random article titles: 500\n"
     ]
    }
   ],
   "source": [
    "data_folder = './data'\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "random_articles_downloaded = 500\n",
    "articles = []\n",
    "for each_index in range(int(random_articles_downloaded // 500)): \n",
    "    articles.append(wikipedia.random(500))\n",
    "articles_titles = [article for i in articles for article in i]\n",
    "print(\"number of random article titles:\", len(set(articles_titles)))       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da46e55",
   "metadata": {},
   "source": [
    "#### Retrieving summary text according to the titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78b86613",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_summary_map = dict()\n",
    "for title_num in range(len(articles_titles)):  \n",
    "    title = articles_titles[title_num]\n",
    "    try:\n",
    "        summary_text = wikipedia.summary(title)\n",
    "    except:\n",
    "        continue\n",
    "    ## Data Cleaning. Replacing newline character,and punctuation with empty space.\n",
    "    preprocessed_text = summary_text.replace(\"\\n\",\"\").replace(\";\",\"\").replace(\"=\",\"\").replace(\"/\",\"\").replace(\"?\",\" \")\n",
    "    titles_summary_map[title] = preprocessed_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a51b53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles_contents = list(titles_summary_map.values())\n",
    "all_titles = list(titles_summary_map.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18d5b1",
   "metadata": {},
   "source": [
    "#### Applying tfidf vectorizer for converting text data into vectors. We are taking maximum features as 1500. We can take also increase this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259da0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tfidf vectorized elements: 1500\n"
     ]
    }
   ],
   "source": [
    "## Initiate TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(input = all_titles_contents, lowercase = True, \n",
    "                              stop_words = \"english\", ngram_range = (1,5) ,max_features = 1500)\n",
    "## Creation of vectors from processed text.\n",
    "titles_contents_matrix = vectorizer.fit_transform(all_titles_contents)\n",
    "\n",
    "print(\"number of tfidf vectorized elements:\", len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7f63b",
   "metadata": {},
   "source": [
    "#### After conversion to vectors,creating cosine similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d00c5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_mat = cosine_similarity(titles_contents_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b731430c",
   "metadata": {},
   "source": [
    "#### Retrieve summary data from user input title and preprocessing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e9be6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_prediciton(input_article_title,recommended_articles):\n",
    "    print(\"Title of the input article is: {}\".format(input_article_title))\n",
    "    summary_text = wikipedia.summary(input_article_title)    \n",
    "    preprocessed_text = summary_text.replace(\"\\n\",\"\").replace(\"?\",\"\").replace(\";\",\"\").replace(\"=\",\"\")\n",
    "    summary_text_processed_vec = vectorizer.transform([preprocessed_text])\n",
    "    cos_similarity = cosine_similarity(summary_text_processed_vec,titles_contents_matrix)\n",
    "    titles_retrived = similarity_mat.argsort()[0][-recommended_articles:][::-1]\n",
    "    cos_similarity.sort()\n",
    "    titles_cos_sim = similarity_mat[0][-recommended_articles:][::-1]\n",
    "    output_titles = [all_titles[each_value] for each_value in titles_retrived ]\n",
    "    print(\"Titles of recommended articles:{}\".format(output_titles))\n",
    "\n",
    "    for i in range(len(output_titles)):\n",
    "        rec_title_value = output_titles[i]\n",
    "        wiki_link_format = \"https://en.wikipedia.org/wiki\"\n",
    "        wiki_article_link_list = wiki_link_format.split('/').copy()\n",
    "        print(\"Recommended article number : {} \\n\".format(i+1))\n",
    "        print(\"Title: {}\\n\".format(output_titles[i]))\n",
    "        link = \"/\".join(wiki_article_link_list+[\"_\".join(output_titles[i].split(\" \"))])\n",
    "        print(\"Link: {}\\n\".format(link))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e08de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the input article is: General Knowledge\n",
      "Titles of recommended articles:['Fædrelandsvennen', 'Extraordinary repatriation', 'Geography of Kazakhstan', 'Århus Stiftstidende', 'Juniper Dunes Wilderness']\n",
      "Recommended article number : 1 \n",
      "\n",
      "Title: Fædrelandsvennen\n",
      "\n",
      "Link: https://en.wikipedia.org/wiki/Fædrelandsvennen\n",
      "\n",
      "Recommended article number : 2 \n",
      "\n",
      "Title: Extraordinary repatriation\n",
      "\n",
      "Link: https://en.wikipedia.org/wiki/Extraordinary_repatriation\n",
      "\n",
      "Recommended article number : 3 \n",
      "\n",
      "Title: Geography of Kazakhstan\n",
      "\n",
      "Link: https://en.wikipedia.org/wiki/Geography_of_Kazakhstan\n",
      "\n",
      "Recommended article number : 4 \n",
      "\n",
      "Title: Århus Stiftstidende\n",
      "\n",
      "Link: https://en.wikipedia.org/wiki/Århus_Stiftstidende\n",
      "\n",
      "Recommended article number : 5 \n",
      "\n",
      "Title: Juniper Dunes Wilderness\n",
      "\n",
      "Link: https://en.wikipedia.org/wiki/Juniper_Dunes_Wilderness\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_input = 'General Knowledge'\n",
    "recommended_article_number = 5\n",
    "article_prediciton(user_input,recommended_article_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "078028b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the input article is: Movie\n",
      "Titles of recommended articles:['Enola Holmes 2', 'The Banshees of Inisherin', 'Terrifier 2']\n",
      "Recommended article number : 1 \n",
      "\n",
      "Title: Enola Holmes 2\n",
      "\n",
      "Link: https://en.wikipedia.org/wiki/Enola_Holmes_2\n",
      "\n",
      "Recommended article number : 2 \n",
      "\n",
      "Title: The Banshees of Inisherin\n",
      "\n",
      "Link: https://en.wikipedia.org/wiki/The_Banshees_of_Inisherin\n",
      "\n",
      "Recommended article number : 3 \n",
      "\n",
      "Title: Terrifier 2\n",
      "\n",
      "Link: https://en.wikipedia.org/wiki/Terrifier_2\n",
      "\n",
      "​\n"
     ]
    }
   ],
   "source": [
    "user_input = 'Movie'\n",
    "recommended_article_number = 3\n",
    "article_prediciton(user_input,recommended_article_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060dafe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
